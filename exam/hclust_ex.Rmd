---
title: "Hierarchical Clustering"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


> **Don't have to assume the number of clusters.**  
> **Just draw "dendogram", and cut the tree at the proper level.**

> **Two main types** of hierarchical clustering :  
> * Agglemerative : All -> merge the closest(similar) pair -> one  
> * Divisive : One -> split -> all cluster  containing one point

> We will focus on agglomerative clustering.  

## Agglomerative clustering

### Algorithm
1. Start with every point in its own cluster  
2. Compute proximities(a proximity matrix *D(i,j)*) between clusters  
3. Merge the two closest clusters  
4. repeat 2 and 3 until only one cluster remains  

How we measure similarity between two : distance  
There are 4 major types to calculate the distance  
1. single : The minimum distance  
2. complete : The maximum distance  
3. average : The average distance between all pairs of points  
4. centroid : The distance between the centroids of each cluster  

** Now, we see the difference between them **


```{r}
library(mvtnorm)
library(ggplot2)
mu1 = c(-1, 0)
mu2 = c(1, 0)
sigma1 = diag(0.4^2, 2)
sigma2 = diag(0.4^2, 2)

x1 = rmvnorm(250, mu1, sigma1)
x2 = rmvnorm(250, mu2, sigma2)
x = rbind(x1, x2)
plot(x)

# Run hierarchical clustering with single (min) linkage
# here min produces counterintuitive results
x_dist = dist(x)
h1 = hclust(x_dist, method='single')
c1 = cutree(h1, 2)

D = data.frame(x, z = c1)
ggplot(D) + geom_point(aes(x=X1, y=X2, col=factor(z)))

# Run hierarchical clustering with complete (max) linkage
h2 = hclust(x_dist, method='complete')
c2 = cutree(h2, 2)
D2 = data.frame(x, z = c2)
ggplot(D2) + geom_point(aes(x=X1, y=X2, col=factor(z)))

# Run hierarchical clustering with average linkage
h3 = hclust(x_dist, method='average')
c3 = cutree(h3, 2)
D3 = data.frame(x, z = c3)
ggplot(D3) + geom_point(aes(x=X1, y=X2, col=factor(z)))

# But here's a different example where max produces counterintuitive results 
# => single method is better in this case
set.seed(84958)
mu1 = c(-1, 0)
mu2 = c(1, 0)
sigma1 = diag(0.1^2, 2)
sigma2 = diag(0.45^2, 2)

x1 = rmvnorm(250, mu1, sigma1)
x2 = rmvnorm(250, mu2, sigma2)
x = rbind(x1, x2)
plot(x)

# Run hierarchical clustering with single (min) linkage
x_dist = dist(x)
h1 = hclust(x_dist, method='single')
c1 = cutree(h1, 2)
D = data.frame(x, z = c1)
ggplot(D) + geom_point(aes(x=X1, y=X2, col=factor(z)))

# Run hierarchical clustering with complete (max) linkage
h2 = hclust(x_dist, method='complete')
c2 = cutree(h2, 2)
D2 = data.frame(x, z = c2)
ggplot(D2) + geom_point(aes(x=X1, y=X2, col=factor(z)))

# Run hierarchical clustering with average linkage
h3 = hclust(x_dist, method='average')
c3 = cutree(h3, 2)
D3 = data.frame(x, z = c3)
ggplot(D3) + geom_point(aes(x=X1, y=X2, col=factor(z)))

```

## Example : Protein data

### Data set 
shows protein sources, such as redmeat, whitemeat, eggs, fish, according to Countries.

```{r}
protein = read.csv("../data/protein.csv", row.names=1)
head(protein)
```

### Hierarchical clustering example

* normalize data set because each feature has a different unit.
```{r}
# Center/scale the data
protein_scaled = scale(protein, center=TRUE, scale=TRUE) 
```

* calculate the distance of each point
```{r}
# Form a pairwise distance matrix using the dist function
protein_distance_matrix = dist(protein_scaled, method='euclidean')

```

* run h-clustering : hclust fn(Method is the type of distance)

```{r}
# Now run hierarchical clustering
hier_protein = hclust(protein_distance_matrix, method='average') 

# Plot the dendrogram
plot(hier_protein, cex=0.8)
```


* clustering by using cutree
```{r}
cluster1 = cutree(hier_protein, k=5) 
summary(factor(cluster1))

which(cluster1 == 1)
which(cluster1 == 2)
which(cluster1 == 3)
```

* cluster using max ("complete") linkage
```{r}
# Using max ("complete") linkage instead
hier_protein2 = hclust(protein_distance_matrix, method='complete') 

# Plot the dendrogram
plot(hier_protein2, cex=0.8)
cluster2 = cutree(hier_protein2, k=5)
summary(factor(cluster2))

# Examine the cluster members
which(cluster2 == 1)
which(cluster2 == 2)
which(cluster2 == 3)
```

## Example : car data

### Data set 
shows cars' type, price and specifications(engine, cylinder, horsepower, MPG, weight, size)

```{r}
cars = read.csv('../data/cars.csv', header=TRUE)
str(cars)
```

### Hierarchical clustering example

* For clustering data, we use only specification features. And do the same procedure as the above data set.

```{r}
# Center and scale the data
X = cars[,-(1:9)]
X = scale(X, center=TRUE, scale=TRUE)

# First form a pairwise distance matrix
distance_between_cars = dist(X)

# Now run hierarchical clustering
h1 = hclust(distance_between_cars, method='complete')

# Plot the dendrogram
plot(h1, cex=0.3)
```

* show the result of 10 clusters

```{r}
# Cut the tree into 10 clusters
cluster1 = cutree(h1, k=10)
summary(factor(cluster1))

# Examine the cluster members
which(cluster1 == 1)
```

